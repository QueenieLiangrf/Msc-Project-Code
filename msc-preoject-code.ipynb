{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nfrom scipy import sparse\nfrom tqdm import tqdm\n\nfrom sklearn.model_selection import GroupShuffleSplit\nfrom bisect import bisect\nfrom sklearn.metrics import mean_squared_error\n\nfrom transformers import AutoConfig\nfrom transformers import AutoTokenizer, AutoModel\n\nimport torch.nn as nn\nimport copy\n\nfrom tqdm import tqdm\nimport sys, os\nfrom transformers import DistilBertModel, DistilBertTokenizer\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport torch\n\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import RobertaTokenizer\n\nimport time\n\nimport matplotlib.pyplot as pl\n","metadata":{"papermill":{"duration":0.122804,"end_time":"2022-05-12T10:15:14.04297","exception":false,"start_time":"2022-05-12T10:15:13.920166","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-09-12T14:07:18.972888Z","iopub.execute_input":"2022-09-12T14:07:18.973676Z","iopub.status.idle":"2022-09-12T14:07:19.089820Z","shell.execute_reply.started":"2022-09-12T14:07:18.973566Z","shell.execute_reply":"2022-09-12T14:07:19.089066Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"pd.options.display.width = 180\npd.options.display.max_colwidth = 120\n\nbert = AutoModel.from_pretrained(\"microsoft/codebert-base\")\ndata_dir = Path('../input/AI4Code')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Read dataset\ndef read_notebook(path):\n    return (\n        pd.read_json(\n            path,\n            dtype={'cell_type': 'category', 'source': 'str'})\n        .assign(id=path.stem)\n        .rename_axis('cell_id')  ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Get the kendall tau corelation values\ndef count_inversions(a):\n    inversions = 0\n    sorted_so_far = []\n    for i, u in enumerate(a):\n        j = bisect(sorted_so_far, u)\n        inversions += i - j\n        sorted_so_far.insert(j, u)\n    return inversions\n\n\ndef kendall_tau(ground_truth, predictions):\n    total_inversions = 0\n    total_2max = 0  # twice the maximum possible inversions across all instances\n    for gt, pred in zip(ground_truth, predictions):\n        ranks = [gt.index(x) for x in pred]  # rank predicted order in terms of ground truth\n        total_inversions += count_inversions(ranks)\n        n = len(gt)\n        total_2max += n * (n - 1)\n    return 1 - 4 * total_inversions / total_2max","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Get the training data\n\nNUM_TRAIN = 1000\n\npaths_train = list((data_dir / 'train').glob('*.json'))[:NUM_TRAIN]\nnotebooks_train = [\n    read_notebook(path) for path in tqdm(paths_train, desc='Train NBs')\n]\ndf = (\n    pd.concat(notebooks_train)\n    .set_index('id', append=True)\n    .swaplevel()\n    .sort_index(level='id', sort_remaining=False)\n)\n","metadata":{"papermill":{"duration":82.291505,"end_time":"2022-05-12T10:16:36.365197","exception":false,"start_time":"2022-05-12T10:15:14.073692","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-09-12T12:33:29.425198Z","iopub.execute_input":"2022-09-12T12:33:29.425413Z","iopub.status.idle":"2022-09-12T12:33:42.374024Z","shell.execute_reply.started":"2022-09-12T12:33:29.425388Z","shell.execute_reply":"2022-09-12T12:33:42.373245Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#Combine all of csv into one table\ndf_orders = pd.read_csv(\n    data_dir / 'train_orders.csv',\n    index_col='id',\n    squeeze=True,\n).str.split()  # Split the string representation of cell_ids into a list\n\ndef get_ranks(base, derived):\n    return [base.index(d) for d in derived]\n\n#nb\n\ndf_orders_ = df_orders.to_frame().join(\n    df.reset_index('cell_id').groupby('id')['cell_id'].apply(list),\n    how='right',\n)\n\nranks = {}\nfor id_, cell_order, cell_id in df_orders_.itertuples():\n    ranks[id_] = {'cell_id': cell_id, 'rank': get_ranks(cell_order, cell_id)}\n\ndf_ranks = (\n    pd.DataFrame\n    .from_dict(ranks, orient='index')\n    .rename_axis('id')\n    .apply(pd.Series.explode)\n    .set_index('cell_id', append=True)\n)\n\n#df_ranks\n\ndf_ancestors = pd.read_csv(data_dir / 'train_ancestors.csv', index_col='id')\n#df_ancestors\n\ndf = df.reset_index().merge(df_ranks, on=[\"id\", \"cell_id\"]).merge(df_ancestors, on=[\"id\"])","metadata":{"execution":{"iopub.status.busy":"2022-09-12T12:34:51.610997Z","iopub.execute_input":"2022-09-12T12:34:51.611717Z","iopub.status.idle":"2022-09-12T12:34:55.261823Z","shell.execute_reply.started":"2022-09-12T12:34:51.611681Z","shell.execute_reply":"2022-09-12T12:34:55.261073Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#Normalize the rangking for each snippet of code\ndf[\"pct_rank\"] = df[\"rank\"] / df.groupby(\"id\")[\"cell_id\"].transform(\"count\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Split the data into 2 sets\nNVALID = 1/3 \n\nsplitter = GroupShuffleSplit(n_splits=1, test_size=NVALID, random_state=0)\n\ntrain_ind, val_ind = next(splitter.split(df, groups=df[\"ancestor_id\"]))\n\ntrain_df = df.loc[train_ind].reset_index(drop=True)\nval_df = df.loc[val_ind].reset_index(drop=True)","metadata":{"papermill":{"duration":1.895199,"end_time":"2022-05-12T10:16:49.969199","exception":false,"start_time":"2022-05-12T10:16:48.074","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-09-12T12:34:59.438852Z","iopub.execute_input":"2022-09-12T12:34:59.439138Z","iopub.status.idle":"2022-09-12T12:35:00.229358Z","shell.execute_reply.started":"2022-09-12T12:34:59.439105Z","shell.execute_reply":"2022-09-12T12:35:00.228583Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#Get the markdown data\ntrain_df_mark = train_df[train_df[\"cell_type\"] == \"markdown\"].reset_index(drop=True)\nval_df_mark = val_df[val_df[\"cell_type\"] == \"markdown\"].reset_index(drop=True)","metadata":{"papermill":{"duration":0.371916,"end_time":"2022-05-12T10:16:52.797271","exception":false,"start_time":"2022-05-12T10:16:52.425355","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-09-12T12:35:06.660123Z","iopub.execute_input":"2022-09-12T12:35:06.660891Z","iopub.status.idle":"2022-09-12T12:35:06.676644Z","shell.execute_reply.started":"2022-09-12T12:35:06.660849Z","shell.execute_reply":"2022-09-12T12:35:06.675888Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"#Get the size of modified networks\n# 1 attention block\nprint(7087872 - sum([2359296,3072,2359296,768,768,768]))\n# 1 intermediate block\nprint(sum([2359296,3072,2359296,768,768,768]))","metadata":{"execution":{"iopub.status.busy":"2022-09-11T13:15:17.354993Z","iopub.execute_input":"2022-09-11T13:15:17.355485Z","iopub.status.idle":"2022-09-11T13:15:17.362084Z","shell.execute_reply.started":"2022-09-11T13:15:17.355448Z","shell.execute_reply":"2022-09-11T13:15:17.361282Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"#Get the parameters of modified networks with different encoder layers\nsize_l = []\nfor i in range(0, 13):\n    print(i, 'encoder layer: ', round((7087872 * i + 38999808) * 0.000001, 2))\n    size_l.append(round((7087872 * i + 38999808) * 0.000001, 2))\nsize_l","metadata":{"execution":{"iopub.status.busy":"2022-09-12T12:09:19.395236Z","iopub.execute_input":"2022-09-12T12:09:19.395505Z","iopub.status.idle":"2022-09-12T12:09:19.411450Z","shell.execute_reply.started":"2022-09-12T12:09:19.395473Z","shell.execute_reply":"2022-09-12T12:09:19.410579Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"#Get the size of modified networks with different encoder layers\nsize_l_d = []\nfor i in range(13):\n    size_l_d.append(round(size_l[12] / size_l[i],2))\nsize_l_d","metadata":{"execution":{"iopub.status.busy":"2022-09-12T12:12:48.620383Z","iopub.execute_input":"2022-09-12T12:12:48.621161Z","iopub.status.idle":"2022-09-12T12:12:48.628507Z","shell.execute_reply.started":"2022-09-12T12:12:48.621112Z","shell.execute_reply":"2022-09-12T12:12:48.627661Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"#Get the parameters of modified networks with different intermediate blocks\nsize_i = []\nfor i in range(0, 13):\n    print(i, 'intermediate block: ', (7087872 - sum([2359296,3072,2359296,768,768,768])) * 12 + 38999808 +  (sum([2359296,3072,2359296,768,768,768])) * i)\n    size_i.append(round(((7087872 - sum([2359296,3072,2359296,768,768,768])) * 12 + 38999808 +  sum([2359296,3072,2359296,768,768,768]) * i) * 0.000001,2))\nsize_i","metadata":{"execution":{"iopub.status.busy":"2022-09-12T11:39:19.672124Z","iopub.execute_input":"2022-09-12T11:39:19.672390Z","iopub.status.idle":"2022-09-12T11:39:19.684185Z","shell.execute_reply.started":"2022-09-12T11:39:19.672360Z","shell.execute_reply":"2022-09-12T11:39:19.683328Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"#Get the parameters of modified networks with different self-attention blocks\nsize_a = []\nfor i in range(0, 13):\n    print(i, 'self-attention block: ', sum([2359296,3072,2359296,768,768,768]) * 12 + 38999808 +  (7087872 - sum([2359296,3072,2359296,768,768,768])) * i)\n    size_a.append(round((sum([2359296,3072,2359296,768,768,768]) * 12 + 38999808 +  (7087872 - sum([2359296,3072,2359296,768,768,768])) * i) * 0.000001,2))\nsize_a","metadata":{"execution":{"iopub.status.busy":"2022-09-12T11:40:37.460496Z","iopub.execute_input":"2022-09-12T11:40:37.460781Z","iopub.status.idle":"2022-09-12T11:40:37.474718Z","shell.execute_reply.started":"2022-09-12T11:40:37.460750Z","shell.execute_reply":"2022-09-12T11:40:37.473983Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"#Get the size of modified networks with different self-attention blocks\nsize_a_d = []\nfor i in range(13):\n    size_a_d.append(round(size_a[12] / size_a[i],2))\nsize_a_d","metadata":{"execution":{"iopub.status.busy":"2022-09-12T11:46:56.598893Z","iopub.execute_input":"2022-09-12T11:46:56.599160Z","iopub.status.idle":"2022-09-12T11:46:56.604240Z","shell.execute_reply.started":"2022-09-12T11:46:56.599130Z","shell.execute_reply":"2022-09-12T11:46:56.603519Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"#Get the different intermediate blocks\ndef deletelayers(model, num_layers_to_keep,n):\n    oldModuleList = model.encoder.layer\n    newModuleList = nn.ModuleList()\n    \n    for j in range(0, num_layers_to_keep):\n        newModuleList.append(oldModuleList[j])\n        \n    for i in range(num_layers_to_keep,11):\n        newModuleList.append(oldModuleList[i].attention)\n        \n    copyofModel = copy.deepcopy(model)\n    copyofModel.encoder.layer = newModuleList\n    \n    return copyofModel","metadata":{"execution":{"iopub.status.busy":"2022-09-12T12:35:32.864907Z","iopub.execute_input":"2022-09-12T12:35:32.869794Z","iopub.status.idle":"2022-09-12T12:35:32.887013Z","shell.execute_reply.started":"2022-09-12T12:35:32.869260Z","shell.execute_reply":"2022-09-12T12:35:32.885793Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#Get the different self-attention blocks\ndef deletelayers_attention(model, num_layers_to_keep,n):\n    oldModuleList = model.encoder.layer\n    newModuleList = nn.ModuleList()\n    \n    for j in range(0, num_layers_to_keep):\n        newModuleList.append(oldModuleList[j])\n        \n    for i in range(num_layers_to_keep,11):\n        newModuleList.append(oldModuleList[i].intermediate)\n        newModuleList.append(oldModuleList[i].output)\n        \n    copyofModel = copy.deepcopy(model)\n    copyofModel.encoder.layer = newModuleList\n    \n    return copyofModel","metadata":{"execution":{"iopub.status.busy":"2022-09-12T12:35:32.895584Z","iopub.execute_input":"2022-09-12T12:35:32.897167Z","iopub.status.idle":"2022-09-12T12:35:33.029643Z","shell.execute_reply.started":"2022-09-12T12:35:32.897128Z","shell.execute_reply":"2022-09-12T12:35:33.027356Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"#Get the different encoder layers\ndef deletelayers_layer(model, num_layers_to_keep,n):\n    oldModuleList = model.encoder.layer\n    newModuleList = nn.ModuleList()\n    \n    for j in range(0, num_layers_to_keep):\n        newModuleList.append(oldModuleList[j])\n        \n    copyofModel = copy.deepcopy(model)\n    copyofModel.encoder.layer = newModuleList\n    \n    return copyofModel","metadata":{"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"##Get the different combination of intermediate blocks and encoder layers\ndef deletelayers_layer_attention(model, num_layers_to_keep,n):\n    oldModuleList = model.encoder.layer\n    newModuleList = nn.ModuleList()\n    \n    for j in range(0, num_layers_to_keep):\n        newModuleList.append(oldModuleList[j])\n        \n    for i in range(num_layers_to_keep,n):\n        newModuleList.append(oldModuleList[i].attention)\n        \n    copyofModel = copy.deepcopy(model)\n    copyofModel.encoder.layer = newModuleList\n    \n    return copyofModel","metadata":{"execution":{"iopub.status.busy":"2022-09-12T12:35:33.120241Z","iopub.execute_input":"2022-09-12T12:35:33.123783Z","iopub.status.idle":"2022-09-12T12:35:33.237690Z","shell.execute_reply.started":"2022-09-12T12:35:33.123731Z","shell.execute_reply":"2022-09-12T12:35:33.235658Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"#Build a model\nclass MarkdownModel(nn.Module):\n    def __init__(self,deletemodel,num_intermediate_to_keep, num_layer):\n        super(MarkdownModel, self).__init__()\n        #self.distill_bert = DistilBertModel.from_pretrained('../input/huggingface-bert-variants/distilbert-base-uncased/distilbert-base-uncased')\n        self.num_intermediate_to_keep = num_intermediate_to_keep \n        self.num_layer = num_layer\n        self.deletemodel = deletemodel\n        self.distill_bert = self.deletemodel(bert, self.num_intermediate_to_keep, self.num_layer)\n        self.top1 = nn.Linear(768, 64)\n        self.top2 = nn.Linear(64, 1)\n\n        self.dropout1 = torch.nn.Dropout(p=0.2)\n        self.dropout2 = torch.nn.Dropout(p=0.2)\n        \n    def forward(self, ids, mask):\n        x = self.distill_bert(ids, mask)[0][:, 0, :]\n        x = self.dropout1(x)\n        x0 = self.top1(x)\n        x = self.dropout2(x0)\n        x = self.top2(x)\n        x = torch.sigmoid(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-09-12T12:35:33.239842Z","iopub.execute_input":"2022-09-12T12:35:33.240882Z","iopub.status.idle":"2022-09-12T12:35:33.335274Z","shell.execute_reply.started":"2022-09-12T12:35:33.240826Z","shell.execute_reply":"2022-09-12T12:35:33.334441Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"#Customize a dataset\nclass MarkdownDataset(Dataset):\n    \n    def __init__(self, df, max_len):\n        super().__init__()\n        self.df = df.reset_index(drop=True)\n        self.max_len = max_len\n        #self.tokenizer = DistilBertTokenizer.from_pretrained('../input/huggingface-bert-variants/distilbert-base-uncased/distilbert-base-uncased', do_lower_case=True)\n        self.tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\", do_lower_case=False)\n\n    def __getitem__(self, index):\n        row = self.df.iloc[index]\n        \n        inputs = self.tokenizer.encode_plus(\n            row.source,\n            None,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding=\"max_length\",\n            return_token_type_ids=True,\n            truncation=True\n        )\n        ids = torch.LongTensor(inputs['input_ids'])\n        mask = torch.LongTensor(inputs['attention_mask'])\n\n        return ids, mask, torch.FloatTensor([row.pct_rank])\n\n    def __len__(self):\n        return self.df.shape[0]\n    ","metadata":{"papermill":{"duration":0.474499,"end_time":"2022-05-12T10:17:01.487031","exception":false,"start_time":"2022-05-12T10:17:01.012532","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-09-12T12:35:33.835502Z","iopub.execute_input":"2022-09-12T12:35:33.835766Z","iopub.status.idle":"2022-09-12T12:35:51.427109Z","shell.execute_reply.started":"2022-09-12T12:35:33.835736Z","shell.execute_reply":"2022-09-12T12:35:51.426294Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"#Get dataloader\ntrain_ds = MarkdownDataset(train_df_mark, max_len=MAX_LEN)\nval_ds = MarkdownDataset(val_df_mark, max_len=MAX_LEN)\n\n\nBS = 32\nNW = 2\nMAX_LEN = 128\n\ntrain_loader = DataLoader(train_ds, batch_size=BS, shuffle=True, num_workers=NW,\n                          pin_memory=False, drop_last=True)\nval_loader = DataLoader(val_ds, batch_size=BS, shuffle=False, num_workers=NW,\n                          pin_memory=False, drop_last=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Get the best learning rate\ndef adjust_lr(optimizer, epoch):\n    if epoch < 1:\n        lr = 5e-5\n    elif epoch < 2:\n        lr = 1e-3\n    elif epoch < 5:\n        lr = 1e-4\n    else:\n        lr = 1e-5\n\n    for p in optimizer.param_groups:\n        p['lr'] = lr\n    return lr\n    \ndef get_optimizer(net):\n    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=3e-4, betas=(0.9, 0.999),\n                                 eps=1e-08)\n    return optimizer\n","metadata":{"papermill":{"duration":0.298424,"end_time":"2022-05-12T10:17:03.132","exception":false,"start_time":"2022-05-12T10:17:02.833576","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-09-12T12:35:51.428747Z","iopub.execute_input":"2022-09-12T12:35:51.428987Z","iopub.status.idle":"2022-09-12T12:35:51.440254Z","shell.execute_reply.started":"2022-09-12T12:35:51.428954Z","shell.execute_reply":"2022-09-12T12:35:51.439527Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"#Train and test data\ndef read_data(data):\n    return tuple(d.cuda() for d in data[:-1]), data[-1].cuda()\n\n\ndef validate(model, val_loader):\n    model.eval()\n    \n    tbar = tqdm(val_loader, file=sys.stdout)\n    \n    preds = []\n    labels = []\n\n    with torch.no_grad():\n        for idx, data in enumerate(tbar):\n            inputs, target = read_data(data)\n            #print(inputs, target)\n\n            pred = model(inputs[0], inputs[1])\n\n            preds.append(pred.detach().cpu().numpy().ravel())\n            labels.append(target.detach().cpu().numpy().ravel())\n    \n    return np.concatenate(labels), np.concatenate(preds)\n\ndef train(model, train_loader, val_loader, epochs):\n    np.random.seed(0)\n    \n    optimizer = get_optimizer(model)\n\n    criterion = torch.nn.MSELoss()\n    \n    for e in range(epochs):\n        start = 0\n        start = time.time()\n        model.train()\n        tbar = tqdm(train_loader, file=sys.stdout)\n        \n        lr = adjust_lr(optimizer, e)\n        \n        loss_list = []\n        preds = []\n        labels = []\n\n        for idx, data in enumerate(tbar):\n            inputs, target = read_data(data)\n\n            optimizer.zero_grad()\n            pred = model(inputs[0], inputs[1])\n\n            loss = criterion(pred, target)\n            loss.backward()\n            optimizer.step()\n            \n            loss_list.append(loss.detach().cpu().item())\n            preds.append(pred.detach().cpu().numpy().ravel())\n            labels.append(target.detach().cpu().numpy().ravel())\n            \n            avg_loss = np.round(np.mean(loss_list), 4)\n\n            tbar.set_description(f\"Epoch {e+1} Loss: {avg_loss} lr: {lr}\")\n        \n        y_train, y_pred_train = validate(model, train_loader)\n        y_val, y_pred = validate(model, val_loader)\n        \n        end = 0\n        end = time.time()\n        print(\"Training MSE:\", np.round(mean_squared_error(y_train, y_pred_train), 4))    \n        print(\"Validation MSE:\", np.round(mean_squared_error(y_val, y_pred), 4))\n        print('Running time:', end - start)\n        print()\n    return model, y_pred, np.round(mean_squared_error(y_train, y_pred_train), 4), np.round(mean_squared_error(y_val, y_pred), 4),end - start","metadata":{"papermill":{"duration":987.160977,"end_time":"2022-05-12T10:33:30.548236","exception":false,"start_time":"2022-05-12T10:17:03.387259","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-09-12T12:36:31.652662Z","iopub.execute_input":"2022-09-12T12:36:31.653163Z","iopub.status.idle":"2022-09-12T12:36:31.669499Z","shell.execute_reply.started":"2022-09-12T12:36:31.653124Z","shell.execute_reply":"2022-09-12T12:36:31.668706Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"#3 intermediate blocks and 8 layers \ntrain_error_n = []\nvali_error_n = []\nkt_error_n = []\ntime_list_n = []\nfor num_layers_to_keep in range(2, 8):\n    model = MarkdownModel(deletelayers_layer_attention, 3, 8)\n    model = model.cuda()\n    model, y_pred, train_MSE, vali_MSE, time_n = train(model, train_loader, val_loader, epochs=1)\n    train_error_n.append(train_MSE)\n    vali_error_n.append(vali_MSE)\n    time_list_n.append(time_n)\n    torch.save(model, 'codebert-trained2.pkl')\n    val_df[\"pred\"] = val_df.groupby([\"id\", \"cell_type\"])[\"rank\"].rank(pct=True)\n    val_df.loc[val_df[\"cell_type\"] == \"markdown\", \"pred\"] = y_pred\n    y_dummy = val_df.sort_values(\"pred\").groupby('id')['cell_id'].apply(list)\n    kt_error_n.append(kendall_tau(df_orders.loc[y_dummy.index], y_dummy))\nprint(kt_error_n)","metadata":{"execution":{"iopub.status.busy":"2022-09-12T12:40:47.044689Z","iopub.execute_input":"2022-09-12T12:40:47.044977Z","iopub.status.idle":"2022-09-12T12:50:30.009399Z","shell.execute_reply.started":"2022-09-12T12:40:47.044947Z","shell.execute_reply":"2022-09-12T12:50:30.008521Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"#Plot the chart about the different numbers of intermediate blocks with 8 Encoder Layers vs Kendall Tau Correlation\nfig, ax = plt.subplots()\nx = np.arange(2,8,1)\nax.plot(x,kt_error_n, label='Kendall Tau Correlation')\nplt.xlim(2,7)\nplt.xlabel('the Numbers of Intermediate Blocks with 8 Encoder Layers')\nplt.ylabel('Kendall Tau Correlation Values')","metadata":{"execution":{"iopub.status.busy":"2022-09-12T13:03:02.090654Z","iopub.execute_input":"2022-09-12T13:03:02.090938Z","iopub.status.idle":"2022-09-12T13:03:02.292537Z","shell.execute_reply.started":"2022-09-12T13:03:02.090907Z","shell.execute_reply":"2022-09-12T13:03:02.291837Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"#Get errors of the different numbers of intermediate blocks with 12 encoder layers\ntrain_error = []\nvali_error = []\nkt_error = []\ntime_list = []\nfor num_layers_to_keep in range(13):\n    #mstart = torch.cuda.memory_allocated(torch.cuda.current_device())\n    model = MarkdownModel(deletelayers, num_layers_to_keep,0)\n    model = model.cuda()\n    model, y_pred, train_MSE, vali_MSE, time_n = train(model, train_loader, val_loader, epochs=1)\n    train_error.append(train_MSE)\n    vali_error.append(vali_MSE)\n    time_list.append(time_n)\n    torch.save(model, 'codebert-trained2.pkl')\n    val_df[\"pred\"] = val_df.groupby([\"id\", \"cell_type\"])[\"rank\"].rank(pct=True)\n    val_df.loc[val_df[\"cell_type\"] == \"markdown\", \"pred\"] = y_pred\n    y_dummy = val_df.sort_values(\"pred\").groupby('id')['cell_id'].apply(list)\n    kt_error.append(kendall_tau(df_orders.loc[y_dummy.index], y_dummy))\n    #mend = torch.cuda.memory_allocated(torch.cuda.current_device())\n    print(\"torch.cuda.memory_allocated: %fKB\"%(torch.cuda.memory_allocated(0)))\n    print(\"torch.cuda.memory_reserved: %fKB\"%(torch.cuda.memory_reserved(0)))\n    print(\"torch.cuda.max_memory_reserved: %fKB\"%(torch.cuda.max_memory_reserved(0)))\n    \n#print(np.mean(train_error)) \n#print(np.mean(vali_error))","metadata":{"execution":{"iopub.status.busy":"2022-09-11T15:28:21.884046Z","iopub.execute_input":"2022-09-11T15:28:21.884654Z","iopub.status.idle":"2022-09-11T15:59:18.229485Z","shell.execute_reply.started":"2022-09-11T15:28:21.884615Z","shell.execute_reply":"2022-09-11T15:59:18.227996Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":137,"outputs":[]},{"cell_type":"code","source":"#Get the time decrese for the different numbers of intermediate blocks with 12 encoder layers\nt_ans = []\nfor i in range(13):\n    t_ans.append((time_list[12] - time_list[i]) / time_list[12])\nt_ans","metadata":{"execution":{"iopub.status.busy":"2022-09-11T16:47:12.555215Z","iopub.execute_input":"2022-09-11T16:47:12.556001Z","iopub.status.idle":"2022-09-11T16:47:12.564021Z","shell.execute_reply.started":"2022-09-11T16:47:12.555952Z","shell.execute_reply":"2022-09-11T16:47:12.562919Z"},"trusted":true},"execution_count":169,"outputs":[]},{"cell_type":"code","source":"#Get the size decrese for the different numbers of intermediate blocks with 12 encoder layers\nsi_ans = []\nsize_i = [67.37,72.09,76.81,81.54,86.26,90.99,95.71,100.43,105.16,109.88,114.61,119.33,124.05]\nfor i in range(12,-1,-1):\n    si_ans.append((size_i[12] - size_i[i]) / size_i[12])\nsi_ans","metadata":{"execution":{"iopub.status.busy":"2022-09-12T04:43:09.365501Z","iopub.execute_input":"2022-09-12T04:43:09.365976Z","iopub.status.idle":"2022-09-12T04:43:09.372528Z","shell.execute_reply.started":"2022-09-12T04:43:09.365942Z","shell.execute_reply":"2022-09-12T04:43:09.371714Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots() \nx = np.arange(0,13,1)\nax.plot(kt_error, label='Kendall Tau Correlation')\nax.plot(t_ans, label='Running Time Increase')\nax.plot(si_ans, label='Size Decrease')\nax.legend(loc='upper left')\nplt.xlabel('the Numbers of Intermediate Blocks')\nplt.ylabel('Values')","metadata":{"execution":{"iopub.status.busy":"2022-09-12T04:50:45.491005Z","iopub.execute_input":"2022-09-12T04:50:45.491302Z","iopub.status.idle":"2022-09-12T04:50:45.726843Z","shell.execute_reply.started":"2022-09-12T04:50:45.491264Z","shell.execute_reply":"2022-09-12T04:50:45.726176Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"##Get errors of the different numbers of encoder layers\ntrain_error_l = []\nvali_error_l = []\nkt_error_l = []\ntime_list_l = []\nfor num_layers_to_keep in range(1,13):\n    model = MarkdownModel(deletelayers_layer, num_layers_to_keep,12)\n    model = model.cuda()\n    model, y_pred, train_MSE, vali_MSE, time_n = train(model, train_loader, val_loader, epochs=1)\n    train_error_l.append(train_MSE)\n    vali_error_l.append(vali_MSE)\n    time_list_l.append(time_n)\n    torch.save(model, 'codebert-trained2.pkl')\n    val_df[\"pred\"] = val_df.groupby([\"id\", \"cell_type\"])[\"rank\"].rank(pct=True)\n    val_df.loc[val_df[\"cell_type\"] == \"markdown\", \"pred\"] = y_pred\n    y_dummy = val_df.sort_values(\"pred\").groupby('id')['cell_id'].apply(list)\n    kt_error_l.append(kendall_tau(df_orders.loc[y_dummy.index], y_dummy))","metadata":{"execution":{"iopub.status.busy":"2022-09-11T17:09:21.907843Z","iopub.execute_input":"2022-09-11T17:09:21.908445Z","iopub.status.idle":"2022-09-11T17:33:17.817760Z","shell.execute_reply.started":"2022-09-11T17:09:21.908407Z","shell.execute_reply":"2022-09-11T17:33:17.816857Z"},"trusted":true},"execution_count":190,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots() \nx = np.linspace(0,13) \nax.plot(kt_error_a)\nax.legend(loc='upper left')\nplt.xlabel('the Numbers of Self-attention Blocks')\nplt.ylabel('Kendall Tau Correlation')","metadata":{"execution":{"iopub.status.busy":"2022-09-12T11:17:55.486439Z","iopub.execute_input":"2022-09-12T11:17:55.486722Z","iopub.status.idle":"2022-09-12T11:17:55.691138Z","shell.execute_reply.started":"2022-09-12T11:17:55.486670Z","shell.execute_reply":"2022-09-12T11:17:55.690468Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots()\ny1 = kt_error_l\nax.plot( y1, label='linear') ","metadata":{"execution":{"iopub.status.busy":"2022-09-10T15:57:41.914029Z","iopub.execute_input":"2022-09-10T15:57:41.914297Z","iopub.status.idle":"2022-09-10T15:57:42.148825Z","shell.execute_reply.started":"2022-09-10T15:57:41.914264Z","shell.execute_reply":"2022-09-10T15:57:42.147813Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"#Get the time decrease for the different numbers of encoder layers\ntime_l_d = []\nfor i in range(13):\n    time_l_d.append(round(time_list_l[12]/time_list_l[i],2))\ntime_l_d","metadata":{"execution":{"iopub.status.busy":"2022-09-12T12:17:24.850614Z","iopub.execute_input":"2022-09-12T12:17:24.850930Z","iopub.status.idle":"2022-09-12T12:17:24.860889Z","shell.execute_reply.started":"2022-09-12T12:17:24.850895Z","shell.execute_reply":"2022-09-12T12:17:24.859976Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}